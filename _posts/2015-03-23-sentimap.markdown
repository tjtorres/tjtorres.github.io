---
layout:     post
title:      "Inside SentiMap.us: Part 1"
subtitle:   "A Pythonic How-To on Twitter Sentiment"
date:       2015-03-23 08:00:00
author:     "TJ Torres"
header-img: "img/sentimap.png"
---


# Introduction #



This is the first in a series of planned posts describing the building of my web app project [SentiMap.us](http://sentimap.us). The basic premise behind SentiMap is to plot localized trends in mood via realtime sentiment analysis of a geotagged Twitter stream. This post will focus mainly on the backend sentiment analysis portion of the project and take you through streaming tweets, feature extraction, and classification. 

Without getting too far into the details just yet, the main functionality behind the sentiment classification, in this case, comes from vectorizing tweets via Word2Vec to construct tweet feature vectors and then training a random forest classifier on a [pre-classified tweet corpus](http://help.sentiment140.com/for-students). We will cover each of these topics in greater detail later, but for now let's focus on the setup.

The entirety of the coding for the first post will be in python, and will use the following libraries:

* Tweepy (Python library for dealing with the Twitter API)
* NLTK (Natural Language Toolkit used for stopwords)
* gensim (Python library containing Word2Vec algorithm)
* Pandas (for managing the Tweet training corpus)
* Scikit-Learn (providing machine learning functionality)

~~~sh
pip install tweepy nltk gensim pandas sklearn
~~~

Note that in order to use NLTK's stop word corpus you must first download it via

~~~python
>>>import nltk
>>>nltk.download()
~~~

With that out of the way, the basic flow follows two main lines:

  
####Training the Classifier####


1. Load the GoogleNews pre-trained Word2Vec vectors. 
2. Import and format/clean the Sentiment140 Twitter Corpus.
3. Vectorize the training data tweets.
4. Train a classifier on the cleaned corpus data (I chose a Random Forest).

####Prediciting Tweet Sentiment From Stream####

1. Grab JSON formatted output from the Stream endpoint of the Twitter API
2. Strip/clean tweet text.
3. Vectorize cleaned tweet.
4. Predict sentiment and store data in database.

##Representing Words as Vectors##

We'll start out by training a machine learning classifier to predict tweet sentiment. The general methodology behind any classification scheme is to take a dataset and transform each data point into a vector that, we hope, represents its most salient features with respect to the task at hand. This process of feature extraction can be nearly trivial for some applications, but in general it represents a substantive obstacle with no unique solution. 

In the case of natural language processing (NLP), the process of feature extraction for use in sentiment analysis is particularly ill-defined. When we, as people, parse a sentence or phrase we can draw upon years of past experience and contextual clues from countless interactions with language to determine whether said phrase is generally positive, negative, or neutral. For instance, most people would generally decide that the sentence "He is not the brightest crayon in the box." is reasonably negative. However, without the idiomatic context, the phrase itself might be viewed as merely neutral. 

Additionally, taking this lack of context to the extreme we might consider the same phrase where even the order of words (that is the contextual clues in the the text itself) were beyond our knowledge. Or perhaps in it's ultimate manifestation, the ordering of letters and characters. It's easy to see the difficulty in deciding anything, other than perhaps letter frequency, about a phrase that has been deconstructed so completely. Yet this is somewhat akin to the world that machines inhabit, having no previous knowledge of language context beyond syntactic rules for translating code documents into binary. 

Thus, while there are certainly more naive approaches to constructing vector representations of documents, the big challenge is then to construct systems which simulate the process of learning contextual knowledge. To that end, we will focus on a system of algorithms, called Word2Vec, designed to extract context clues for specific words via the analysis of massive datasets of text. 

As much as I'd love to speak a bit about how Word2Vec works (perhaps a future blog post) in gritty technical detail, for now I will point you to a [blog post written by a friend of mine](http://technology.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/) which is quite accessible to the lay-person, but contains a large number of technical links at the bottom for those who wish to delve further. Suffice it to say here that Word2Vec "learns" N-dimensional vector representations of words by analyzing and associating words that are less than a certain distance from them in the corpus of training text. At the end of the day we want two words to have vectors that are "close" to each other when it is probabilistically favorable for them to be near each other in a system of documents. For example, a decent metric for determining the "closeness" of vectors is the angle between them which can be easily obtained through use of the inner (or dot) product. Consequently, one might expect that "lion" and "giraffe" are often mentioned within close proximity to one another and thus the angle between their vector representations should be close to zero. 

Given the amount of text needed to train these models, it is often advantageous to use large corpuses of words with their pre-trained vectors representations. As the original implementation of Word2Vec was invented by a Google engineer, the codebase, as well as [pre-trained vector sets](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) can be found on their site [here](https://code.google.com/p/word2vec/). For this example we will use their vectors that have been trained on GoogleNews stories. 

Since our vectors are pre-trained we will just need to load the binary file into gensim's Word2Vec implementation. Vectors for given words are then easily accessible by using the word strings as keys on our model instance. Then we will construct a function to take a string with only letters and whitespace and remove common words of little contextual meaning, called stop words, then output a vector based on the average of the set of words in the string.

~~~python
import gensim as gs

model = gs.models.Word2Vec.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)

def phrase2vec(phrase):
    phrase = phrase.lower().split()
    phrase_fil = [w for w in phrase if not w in stop_set]
    size = 0
    vec = np.zeros(300)
    for word in phrase_fil:
        try:
            vec= np.add(vec,model[word])
            size+=1
        except:
            pass
    if size==0:
        size=1
    return np.divide(vec,size)

~~~





