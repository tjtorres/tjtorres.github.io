---
layout:     post
title:      "Inside SentiMap.us: Part 1"
subtitle:   "A Pythonic How-To on Twitter Sentiment"
date:       2015-03-23 08:00:00
author:     "TJ Torres"
header-img: "img/sentimap.png"
---


# Introduction #



This is the first in a series of planned posts describing the building of my web app project [SentiMap.us](http://sentimap.us). The basic premise behind SentiMap is to plot localized trends in mood via realtime sentiment analysis of a geotagged Twitter stream. This post will focus mainly on the backend sentiment analysis portion of the project and take you through streaming tweets, feature extraction, and classification. 

Without getting too far into the details just yet, the main functionality behind the sentiment classification, in this case, comes from vectorizing tweets via Word2Vec to construct tweet feature vectors and then training a random forest classifier on a [pre-classified tweet corpus](http://help.sentiment140.com/for-students). We will cover each of these topics in greater detail later, but for now let's focus on the setup.

The entirety of the coding for the first post will be in python, and will use the following libraries:

* Tweepy (Python library for dealing with the Twitter API)
* NLTK (Natural Language Toolkit used for stopwords)
* gensim (Python library containing Word2Vec algorithm)
* Pandas (for managing the Tweet training corpus)
* Scikit-Learn (providing machine learning functionality)

~~~sh
pip install tweepy nltk gensim pandas sklearn
~~~

Note that in order to use NLTK's stop word corpus you must first download it via

~~~python
>>>import nltk
>>>nltk.download()
~~~

With that out of the way, the basic flow follows two main lines:

  
####Training the Classifier####


1. Load the GoogleNews pre-trained Word2Vec vectors. 
2. Import and format/clean the Sentiment140 Twitter Corpus.
3. Vectorize the training data tweets.
4. Train a classifier on the cleaned corpus data (I chose a Random Forest).

####Prediciting Tweet Sentiment From Stream####

1. Grab JSON formatted output from the Stream endpoint of the Twitter API
2. Strip/clean tweet text.
3. Vectorize cleaned tweet.
4. Predict sentiment and store data in database.

##Representing Words as Vectors##

We'll start out by training a machine learning classifier to predict tweet sentiment. The general methodology behind any classification scheme is to take a dataset and transform each data point into a vector that, we hope, represents its most salient features with respect to the task at hand. This process of feature extraction can be nearly trivial for some applications, but in general it represents a substantive obstacle with no unique solution. 

In the case of natural language processing (NLP), the process of feature extraction for use in sentiment analysis is particularly ill-defined. When we, as people, parse a sentence or phrase we can draw upon years of past experience and contextual clues from countless interactions with language to determine whether said phrase is generally positive, negative, or neutral. For instance, most people would generally decide that the sentence "He is not the brightest crayon in the box." is reasonably negative. However, without the idiomatic context, the phrase itself might be viewed as merely neutral. 

Additionally, taking this lack of context to the extreme we might consider the same phrase where even the order of words (that is the contextual clues in the the text itself) were beyond our knowledge. Or perhaps in it's ultimate manifestation, the ordering of letters and characters. It's easy to see the difficulty in deciding anything, other than perhaps letter frequency, about a phrase that has been deconstructed so completely. Yet this is somewhat akin to the world that machines inhabit, having no previous knowledge of language context beyond syntactic rules for translating code documents into binary. 

Thus, while there are certainly more naive approaches to constructing vector representations of documents, the big challenge is then to construct systems which simulate the process of learning contextual knowledge. To that end, we will focus on a system of algorithms, called Word2Vec, designed to extract context clues for specific words via the analysis of massive datasets of text. 

As much as I'd love to speak a bit about how Word2Vec works (perhaps a future blog post) in gritty technical detail, for now I will point you to a [blog post written by a friend of mine](http://technology.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/) which is quite accessible to the lay-person, but contains a large number of technical links at the bottom for those who wish to delve further. Suffice it to say here that Word2Vec "learns" N-dimensional vector representations of words by analyzing and associating words that are less than a certain distance from them in the corpus of training text. At the end of the day we want two words to have vectors that are "close" to each other when it is probabilistically favorable for them to be near each other in a system of documents. For example, a decent metric for determining the "closeness" of vectors is the angle between them which can be easily obtained through use of the inner (or dot) product. Consequently, one might expect that "lion" and "giraffe" are often mentioned within close proximity to one another and thus the angle between their vector representations should be close to zero. 

Given the amount of text needed to train these models, it is often advantageous to use large corpuses of words with their pre-trained vectors representations. As the original implementation of Word2Vec was invented by a Google engineer, the codebase, as well as [pre-trained vector sets](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) can be found on their site [here](https://code.google.com/p/word2vec/). For this example we will use their vectors that have been trained on GoogleNews stories. 

Since our vectors are pre-trained we will just need to load the binary file into gensim's Word2Vec implementation. Vectors for given words are then easily accessible by using the word strings as keys on our model instance. Then we will construct a function to take a string with only letters and whitespace and remove common words of little contextual meaning, called stop words, then output a vector based on the average of the set of vectors that correspond to words in the string.

~~~python
import gensim as gs
import numpy as np

model = gs.models.Word2Vec.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)

def phrase2vec(phrase):
    phrase = phrase.lower().split()
    phrase_fil = [w for w in phrase if not w in stop_set]
    size = 0
    vec = np.zeros(300)
    for word in phrase_fil:
        try:
            vec= np.add(vec,model[word])
            size+=1
        except:
            pass
    if size==0:
        size=1
    return np.divide(vec,size)

~~~

We now have a nice function for vectorizing tweets, once they have been properly formatted.


##Cleaning the Data##


Next, we'll need to get some training data to run our feature extraction method on. Though there are several Twitter sentiment corpora on the web, many of them sacrifice sample size for accuracy of classification of the training set. Since we are dealing with the potentially very powerful, but somewhat nebulous, concept of word vectors we should choose instead to err on the side of a much larger training set, with perhaps a slight hit to classification accuracy. This can be accomplished by creating heuristics for classification rather than relying on manual techniques. The [Sentiment 140 Corpus](http://help.sentiment140.com/for-students/) constructs its sentiment classification heuristic on tweets that contain emoticons. Of the tweets that contain emoticons, those that contain mostly positive emoticons are classified as such, and the same goes for the negative set. 

With this heuristic one can immediately automate the data classification task for the training set, which allows for a far greater sample size (in this case 1.6M tweets), at the expense of a possible drop in accuracy. 

To analyze and clean our data we're going to work with Pandas, but first let's take a brief look at the format of the training data.

~~~sh
$head -10 training.1600000.processed.noemoticon.csv

"0","1467810369","Mon Apr 06 22:19:45 PDT 2009","NO_QUERY","_TheSpecialOne_","@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D"
"0","1467810672","Mon Apr 06 22:19:49 PDT 2009","NO_QUERY","scotthamilton","is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!"
"0","1467810917","Mon Apr 06 22:19:53 PDT 2009","NO_QUERY","mattycus","@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds"
"0","1467811184","Mon Apr 06 22:19:57 PDT 2009","NO_QUERY","ElleCTF","my whole body feels itchy and like its on fire "
$
~~~

From the first few lines you can see the general format of the data and should notice several things. For instance the training data file doesn't have a header, so we'll have to name the columns ourselves. Also, the text of the tweets has been formatted to remove emoticons, but still contains plenty of errant strings like URLs and @name tags we'll need to remove before proceeding to vectorize them. 

First let's import the csv file and store it as a dataframe.

~~~python
import pandas as pd
name_list = ['sentiment','id','time','query','user','text']
df = pd.read_csv("training.1600000.processed.noemoticon.csv",\
                 header=None, names= name_list)
~~~





