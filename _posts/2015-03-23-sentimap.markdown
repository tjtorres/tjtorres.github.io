---
layout:     post
title:      "Inside SentiMap.us: Part 1"
subtitle:   "A How-To on Twitter Sentiment"
date:       2015-03-23 08:00:00
author:     "TJ Torres"
header-img: "img/sentimap.png"
---


# Introduction #

This is the first in a series of planned future posts describing the building of my web app project [SentiMap.us](http://sentimap.us). The basic premise behind SentiMap is to plot localized trends in mood via realtime sentiment analysis of a geotagged Twitter stream. This post will focus mainly on the backend sentiment analysis portion of the project and take you through streaming tweets, feature extraction, and classification. 

Without getting too far into the details just yet, the main functionality behind the sentiment classification, in this case, comes from vectorizing tweets via Word2Vec to construct tweet feature vectors and then training a random forest classifier on a [pre-classified tweet corpus](http://help.sentiment140.com/for-students). We will cover each of these topics in greater detail later, but for now let's focus on the setup.

To that end we will start off needing a few tools:

* Tweepy (Python library for dealing with the Twitter API)
* NLTK (Natural Language Toolkit used for stopwords)
* gensim (Python library containing Word2Vec algorithm)
* Pandas (for managing the Tweet training corpus)
* Scikit-Learn (providing machine learning functionality)

The basic flow follows two main lines:
  
####Training the Classifier####

1. Load the GoogleNews pre-trained Word2Vec vectors. 
2. Import and format/clean the Sentiment140 Twitter Corpus.
3. Vectorize the training data tweets.
4. Train a classifier on the cleaned corpus data(I chose a Random Forest).

####Prediciting Tweet Sentiment From Stream####

1. Grab JSON formatted output from the Stream endpoint of the Twitter API
2. Strip/clean tweet text.
3. Vectorize cleaned tweet.
4. Predict sentiment and store data in database.


Test.


```python

def function():
	here = []
	for i in range(7):
		here.append(i)

function()
```
